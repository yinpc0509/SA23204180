---
title: "Homework"
author: "SA23204180 Yin Pengcheng"
date: "2023-12-04"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# HW0 

## Question 
Use knitr to produce at least 3 examples. For each example,
texts should mix with figures and/or tables. Better to have
mathematical formulas.

## Answer

### Example 1

$R^2$represents the coefficient of determination, and it is computed as follows；
$$R^2 = 1 - \frac{SS_{\text{residual}}}{SS_{\text{total}}}$$

```{r}
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
summary(lm.D9)$coef
```
The $R^2$ is `r summary(lm.D9)$r.squared`

### Example 2

Create a table and a hist using the inner dataset iris. 

```{r}
library(ggplot2)
head(iris)

ggplot(iris, aes(x = Petal.Length, fill = Species)) +
  geom_histogram(binwidth = 0.2) +
  labs(title = "Petal Length Distribution", x = "Petal Length", y = "Frequency")

```

# HW1

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(114514)
```


## Question{#question}

1. 利用逆变换法复现sample(replace=True)的部分功能

2. (3.2) The standard Laplace distribution has density $f(x) = \frac{1}{2}e^{−|x|}, x ∈ R$ .Use the inverse transform method to generate a random sample of size 1000 from this
distribution. Use one of the methods shown in this chapter to compare the
generated sample to the target distribution.

3. (3,7) Write a function to generate a random sample of size n from the Beta(a, b) distribution by the acceptance-rejection method. Generate a random sample of size 1000 from the Beta(3,2) distribution. Graph the histogram of the sample with the theoretical Beta(3,2) density superimposed.  

4. (3.9) The rescaled Epanechnikov kernel [85] is a symmetric density function
$$f_e(x)=\frac{3}{4}(1-x^2),|x|\leq 1$$
Devroye and Gy¨ orfi [71, p. 236] give the following algorithm for simulation
from this distribution. Generate iid U1, U2, U3 ∼ Uniform(−1, 1). If |U3| ≥
|U2| and |U3| ≥ |U1|, deliver U2; otherwise deliver U3. Write a function
to generate random variates from fe, and construct the histogram density
estimate of a large simulated random sample. 

5. (3.10)Prove that the algorithm given in Exercise 3.9 generates variates from the
density $f_e$ 

## Answer
### Question 1{}

函数的具体实现如下：
```{r}
my_sample <- function(x,size=length(x),prob=rep(1,length(x))) {
  # 防止输入概率和不为1 进行”归一化“
  prob <- prob/sum(prob)
  # 逆变换法
  cp <- cumsum(prob)
  U = runif(size)
  r <- x[findInterval(U,cp)+1]
  return(r)
}
```

分别对不指定概率分布与指定概率分布的情形产生实例验证结果：
```{r}
# Exp 1
table(my_sample(1:3,10000))
# Exp 2
table(my_sample(1:3,10000,prob = c(.2,.3,.5)))
```

### Question 2{}

$f(x) = 0.5e^{−|x|}, x ∈ R$ ,其CDF的逆函数为: 
$$F^{-1}(u)=
\begin{equation}
    \left\{
    \begin{aligned}
        ln(2u), u<0.5 \\
        -ln(2(1-u)), u>0.5
    \end{aligned}
    \right.
\end{equation}$$

```{r}
n <- 1000
u <- runif(n)
u[u<.5] <- log(2*u[u<.5])
u[u>=.5] <- -log(2-2*u[u>=.5])
hist(u, prob = TRUE, main = expression(f(x)==0.5*exp(-abs(x))),xlim = c(-5,5))
y <- seq(-5, 5, .01)
lines(y, exp(-abs(y))/2)
```

### Question 3{}

若 $a,b<1$,当 $x\rightarrow0\quad or\quad x\rightarrow1$时 $f(x)$ 会趋近 $\infty$ , 因此假设 $a,b\geq1$。
可以通过 optimize() 得到$f(x)$的最大值.记为$x_0$
选择$g(x)=1,0<x<1$ ， $c=x_0+0.5$.

```{r}
gamma <- function(x){
  return(factorial(x-1))
}

beta_ar <- function(n, a, b){
  k = 0
  y = numeric(n) 
  #Beta(a,b)的密度函数
  fbeta <- function(x){
    return(gamma(a+b)/gamma(a)/gamma(b)*x^(a-1)*(1-x)^(b-1))
  }
  x0 = as.numeric(optimize(fbeta,lower = 0, upper = 1, maximum = TRUE)[[1]])
  while(k < n){
    u = runif(1)
    x = runif(1)
    if((fbeta(x)/(fbeta(x0)+0.5)) > u){
      k = k+1
      y[k] = x
    }
  }
  return(y)
}
```

```{r}
# set.seed(114514)
n = 1000
x = beta_ar(n, 3, 2)
y = seq(0, 1, 0.01)
hist(x, probability = TRUE, main = "Beta(3,2)", ylim = c(0,2))
lines(y, dbeta(y, 3, 2))
```


### Question 4{}

由所给算法进行实现：
```{r}
n <- 1000
DG_simulation <- function(n){
  k <- 0
  while(k<n){
    k <- k+1
    u1 <- runif(1,-1,1)
    u2 <- runif(1,-1,1)
    u3 <- runif(1,-1,1)
    if(abs(u3)>=abs(u2) & abs(u3)>=abs(u2)){
      x[k] <- u2
    } else {
      x[k] <- u3
    }    
  }
  return(x)
  }
x <- DG_simulation(10000)
y = seq(-1, 1, 0.01)
hist(x,main = expression(f(x)==0.75*(1-x^2)),probability = TRUE)
lines(y, 0.75*(1-y^2))

```


### Question 5{}

由对称性，不妨假设所取均为[0,1]上均匀分布。不难理解，该算法所取为次序统计量中非最大元素，n个元素中第k大的次序统计量的密度函数公式为$$f_k(x)=\frac{n!}{(k-1)!(n-k)!}F(x)^{k−1}(1−F(x))^{n−k}(x)f(x)$$,该算法中$n=3$,
$$f_1(x)=3(1-x)^2,f_2(x)=6x(1-x), 0\leq x\leq1$$
$$f(x)=\frac{1}{2}(f_1(x)+f_2(x))=\frac{3}{2}(1-x^2),0\leq x\leq1$$
再由对称性可知,(3.9)中算法所取变量的密度函数为$$f_e(x)=\frac{3}{4}(1-x^2),|x|\leq 1$$

# HW2


## Question{}

1. 
• Proof that what value $l/d:=\rho$ should take to minimize the asymptotic variance of $\hat{\pi}=\frac{2l}{d\hat{p}}$? ($m\sim B(n,p)$, using δ method)
• Take three different values of $\rho$ ($0 ≤ ρ ≤ 1$, including $ρ_{min}$) and use Monte Carlo simulation to verify your answer. ($n = 10^6$, Number of repeated simulations $K = 100$)

2. (5.6) In Example 5.7 the control variate approach was illustrated for Monte Carlo
integration of
$$\theta=\int_{0}^{1}e^xdx.$$
Now consider the antithetic variate approach. Compute $Cov(e^U,e^{1−U})$ and
$Var(e^U+e^{1−U})$, where $U ∼ Uniform(0,1).$ What is the percent reduction in
variance of $\hat\theta$ that can be achieved using antithetic variates (compared with
simple MC)?

3. (5,7) Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate $\theta$ by the
antithetic variate approach and by the simple Monte Carlo method. Compute
an empirical estimate of the percent reduction in variance using the antithetic
variate. Compare the result with the theoretical value from Exercise 5.6. 



## Answer
### Question 1{}
$\hat{p}=\frac{m}{n},m\sim B(n,p)$
$$\hat{\pi}=\frac{2l}{d\hat{p}}=\frac{2ln}{dm},p=\frac{2\rho}{\pi}$$
$$g(\theta)=\frac{1}{\theta},g'(\theta)=-\frac{1}{\theta^2}$$

$$Var(\hat{\pi})=4\rho^2\frac{\sigma^2[g'(\theta)]^2}{n}=\frac{\pi^3}{2n^2}\frac{1-2\rho/\pi}{\rho}$$

可以看出，渐近方差随$\rho$的增大而单调递减,由约束规则可知$\rho_{min}=1$

```{r}
rho <- c(0.5, 0.8, 1)
for(i in rho){
  d <- 1
  l <- d*i
  m <- 1e6
  pihat <- c()
  for (j in 1:100) {
    X <- runif(m,0,d/2)
    Y <- runif(m,0,pi/2)
    pihat[j] <- 2*l/d/mean(l/2*sin(Y)>X)
  }
  
  cat("When rho = ",i,"Var is ",var(pihat),"\n")
  
}
```


### Question 2{}

$$\begin{aligned}
Cov(e^U,e^{1−U})
& = E(e^Ue^{1-U})-E(e^U)E(e^{1-U})\\
& = e-(e-1)^2\\
& = -e^2+3e-1, \\
Var(e^U+e^{1−U})
& = Cov(e^U+e^{1−U},e^U+e^{1−U})\\
& = Var(e^U)+2Cov(e^U,e^{1−U})+Var(e^{1-U})\\
& = \frac{e^2-1}{2}-(e-1)^2+2(-e^2+3e-1)+\frac{e^2-1}{2}-(e-1)^2\\
& = -3e^2+10e-5.
\end{aligned}$$


对于简单估计,$Var(\hat\theta)=\frac{Var(e^U)}{m}$.\
对于对偶估计,$Var(\hat\theta')=\frac{Var(e^U+e^{1−U})}{2m}$.\
方差减小程度为：
$$100\%*(1-\frac{Var(e^U+e^{1−U})}{2Var(e^U)})=100\%*(1-\frac{-3e^2+10e-5}{-e^2+4e-3})
\approx96.767\%.$$

### Question 3{}

```{r}
MC_anti <- function(n, anti=FALSE){
  u = runif(n/2)
  if(anti) {
    v = 1-u
    return((mean(exp(u))+mean(exp(v)))/2)
    }
  else {
    v = runif(n/2)
    u = c(u,v)
    return(mean(exp(u)))
    }
}
set.seed(0)
m = 1e4
v1 <- v2 <- numeric(1000)
for(i in 1:1000){
 v1[i] = MC_anti(m)
 v2[i] = MC_anti(m,TRUE)
}
var1 = var(v1)
var2 = var(v2)
cat("Simple var:",var1,"\t Anti var: ",var2)
cat("\nThe variance reduction is: ",(var1-var2)/var1)
```

由结果可以看出，与5.6中理论计算相符。

# HW3


## Question{}

1. 
$Var(\hat{θ}^M) = \frac{1}{Mk} \sum_{i=1}^k\sigma_i^2+ Var(\theta_I)=Var(\hat{θ}^S)+ Var(\theta_I)$, where $θi = E[g(U) | I = i], σi2 = V ar[g(U) | I = i]$
and I takes uniform distribution over $\{1, . . . , k\}$.
Proof that if g is a continuous function over $(a, b)$, then $Var(\hat{θ}^S)/V ar(\hat{θ}^M) → 0$ as $b_i − a_i → 0$ for all
$i = 1, . . . , k$.

2. (5.13) Find two importance functions f1 and f2 that are supported on (1, ∞) and
are ‘close’ to $g(x) = \frac{x^2}{\sqrt{2\pi}}e^{−x^2/2}, x > 1$.
Which of your two importance functions should produce the smaller variance
in estimating
$\int_1^\infty \frac{x^2}{\sqrt{2\pi}}e^{−x^2/2}dx$
by importance sampling? Explain.?

3. (5,14) Obtain a Monte Carlo estimate of
$\int_1^\infty \frac{x^2}{\sqrt{2\pi}}e^{−x^2/2}dx$
by importance sampling. 

4. (5.15)
Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

5. (6.5) 
Suppose a 95% symmetric t-interval is applied to estimate a mean, but the
sample data are non-normal. Then the probability that the confidence interval
covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment
to estimate the coverage probability of the t-interval for random samples of
$χ^2(2)$ data with sample size n = 20. Compare your t-interval results with the
simulation results in Example 6.4. (The t-interval should be more robust to
departures from normality than the interval for variance.)

6. (6.A)
Use Monte Carlo simulation to investigate whether the empirical Type I error rate of the t-test is approximately equal to the nominal significance level
$α$, when the sampled population is non-normal. The t-test is robust to mild
departures from normality. Discuss the simulation results for the cases where
the sampled population is (i) $χ^2(1)$, (ii) Uniform(0,2), and (iii) Exponential(rate=1). In each case, test $H0 : µ = µ0 ~vs~ H0 : µ ∕= µ0,$ where µ0 is the
mean of $χ^2(1)$, Uniform(0,2), and Exponential(1), respectively.


## Answer
### Question 1{}

we want to prove that $\frac{Var(\hat{\theta}^S)}{Var(\hat{\theta}^M)} \rightarrow 0$ as $(b_i - a_i) \rightarrow 0$ for all $i = 1, \ldots, k$.

 As $b_i - a_i$ gets closer to 0, since $g$ is a continuous function over $(a, b)$, as the intervals become smaller, the variation in $g(U)$ for each $i$ also becomes smaller. Therefore, $\sigma_i^2$ will tend to 0 as $(b_i - a_i) \rightarrow 0$ for all $i$. 



### Question 2{}

It's evident that the function $g(x)$ closely resembles the probability density function of the standard normal distribution. Therefore, we can define $f_1(x)$ as follows $$f_1(x)=\frac{1}{\sqrt{2\pi}} e^{−x^2/2}.$$ Additionally, we can use the probability density function of the Gamma distribution for $f_2$. The density function of a $Gamma(3,2)$ distribution is given by:
$$f_2(x)=\frac{1}{16}x^2e^{-x/2}I_{(0,\infty)}(x).$$

+ Here are two figures illustrating the functions $g, f_1, f_2$. 
```{r}
g <- function(x){
  x^2*exp(-x^2/2)/sqrt(2*pi)*(x>=1)
}
x = seq(1,8,0.01)
gs <- c(expression(g(x)),expression(f1(x)),expression(f2(x)))
par(mfrow=c(1,2))
# figure of g, f1, f2
plot(x, g(x), type="l", ylab="", ylim=c(0,0.6), lwd = 2, col=1)
lines(x, dnorm(x), lwd=2, col=2)
lines(x, dgamma(x,3,2), lwd=2, col=3)
legend("topright", legend = gs, lty=1, lwd=2, inset = 0.02,col=1:3)

# figure of g/f1, g/f2
plot(x, g(x)/dnorm(x), type="l", ylab="", ylim=c(0,5), lwd = 2, col=2)
lines(x, g(x)/dgamma(x,3,2), lwd=2, col=3)
legend("topright", legend = gs[-1], lty=1, lwd=2, inset = 0.02,col=2:3)
```

From the figures, we can make an inference that $f_2$ may provide a more precise estimation, as the ratio $g/f_2$ appears closer to a constant compared to $g/f_1$, which is a quadratic function. Moreover, $f_1$ covers a broader range, and many simulated values would contribute zeros to the sum, resulting in inefficiency.

Now, let's proceed with a simulation:
```{r}
set.seed(0)
m = 1e4
theta <- se <- numeric(2)
# using f1
x <- rnorm(m) 
fg <- g(x) / dnorm(x)
theta[1] <- mean(fg)
se[1] <- sd(fg)
# using f2
x <- rgamma(m,3,2) 
fg <- g(x) / dgamma(x,3,2)
theta[2] <- mean(fg)
se[2] <- sd(fg)
rbind(theta, se)
```

```{r}
se^2
```

After conducting the simulation, we can affirm that $f_2$ offers a superior estimation of $\theta$

### Question 3{}
The mass function of the original Gamma distribution is 
$$f(x | \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\beta x}, \quad x > 0$$
we need to change it into 
$$f(x | \alpha', \beta') = \frac{\beta'^{\alpha'}}{\Gamma(\alpha')} (x-1)^{\alpha' - 1} e^{-\beta' (x-1)}, \quad x > 1$$
$α$ and $β$ are the original shape and scale parameters, while $α'$ and $β'$ are the redefined shape and scale parameters, to make the distribution applicable within the interval $(1, ∞)$.

```{r}
# Parameters for the gamma distribution
alpha <- 3  # Shape parameter
beta <- 1   # Scale parameter

# Number of samples
N <- 100000

samples <- rgamma(N, shape = alpha, rate = beta)
estimate <- (mean((samples^2 * gamma(alpha)) / beta^alpha))

# Print the estimate
cat("Monte Carlo Estimate:", estimate, "\n")


```



### Question 4{}
+ Example 5.10

```{r}
set.seed(0)
m <- 1e6
est1 <- sd1 <- 0
g <- function(x){
  exp(-x - log(1+x^2)) * (x > 0) * (x < 1)
}
u <- runif(m) #f3, inverse transform method
x <- - log(1 - u * (1 - exp(-1)))
fg <- g(x) / (exp(-x) / (1 - exp(-1)))
est1 <- mean(fg)
sd1 <- sd(fg)
```

+ Example 5.13

Let $f(x)=\frac{e^{-x}}{1-e^{-1}}$, $F(x)=\int f(x)=\frac{1-e^{-x}}{1-e^{-1}}$. Then $F^{-1}(x)=-log[1-(1-e^{-1})x)]$. We divide the real line into $k=5$ intervals 
$I_j=\{x:a_{j-1}\leq x<a_j\}$ and $a_j=F^{-1}(\frac{j}{k}),j=1,\dots,k$. On each subinterval, the conditional density $f_j$ of $X$ is 
$$f_j(x)=f_{X|I_j}(x|I_j)=\frac{f(x,a_{j-1}\leq x<a_j)}{P(a_{j-1}\leq x<a_j)}=kf(x),\quad a_{j-1}\leq x<a_j.$$

```{r}
M <- 10000
k <- 5 
m <- M/k # replicates per stratum
T1 <- T2 <- numeric(k)
est2 <- numeric(2)
fj <- function(x)5*exp(-x)/(1-exp(-1)) # fj(x)
g <- function(x)exp(-x)/(1+x^2)
```

For each subinterval $I_j$, we use inverse transform method to generate samples from $f_j(x)$. Since 
$$\begin{aligned}
F_j(x)
&=\int_{a_{j-1}}^{x}f_j(x)dx\\
&=5F(x)-5F(a_{j-1})\\
&=5(F(x)-j+1),
\end{aligned}$$
we have $F^{-1}_j(x)=-log[1-(1-e^{-1})(x+j-1)]$. \
Let $\sigma_j^2=Var(g_j(X)/f_j(X))$. For each $j = 1,...,k$, we simulate an importance sample size $m$, compute the importance sampling estimator $\hat\theta_j$ of $\theta_j$ on the $j^{th}$ subinterval, and compute $\displaystyle\hat\theta^{SI}=\sum_{j=1}^k{\hat\theta_j}$.Then by independence of
$\hat\theta_1,..., \hat\theta_k$,
$$\displaystyle Var(\hat\theta^{SI})=Var(\sum_{j=1}^k{\hat\theta_j})=\sum_{j=1}^k{\frac{\sigma_j^2}{m}}=\frac{1}{m}\sum_{j=1}^k{\sigma_j^2}.$$

```{r}
set.seed(0)
for(j in 1:k){
  u = runif(m)
  x = -log(1-(1-exp(-1))*(u+j-1)/5) # inverse transform method
  T1[j] = mean(g(x)/fj(x))
  T2[j] = var(g(x)/fj(x))
}
est2[1] = sum(T1) 
est2[2] = sum(T2)
round(c(est1,est2[1]),4)
round(c(sd1,sqrt(est2[2])),5)
```

From the results, it's oblivious that the stratified importance sampling estimate is better because its variance is much smaller than the result of Example 5.10.



### Question 5{}

```{r}
cl <- 0.95
s <- 20
n <- 10000
true_mean <- 2  

successful_intervals <- 0

for (i in 1:n) {
  x <- rchisq(s, df = 2)
  
  sample_mean <- mean(x)
  sample_sd <- sd(x)
  
  moe <- qt((1 + cl) / 2, df = s - 1) * (sample_sd / sqrt(s))
  interval <- c(sample_mean - moe, sample_mean + moe)
  
  if (true_mean >= interval[1] && true_mean <= interval[2]) {
    successful_intervals <- successful_intervals + 1
  }
}

c_p <- successful_intervals / n

cat("Estimated Coverage Probability:", c_p, "\n")



```

The t-interval should be more robust to departures from normality than the interval for variance.


### Question 6{}

```{r}
n <- 10000
alpha <- 0.05

true_mean_chi_sq <- 1
true_mean_uniform <- 1
true_mean_exponential <- 1

rej_chi_sq <- 0
rej_uniform <- 0
rej_exponential <- 0

# Perform Monte Carlo simulations for each case
for (i in 1:n) {
  
  sample_chi_sq <- rchisq(30, df = 1)  
  sample_uniform <- runif(30, min = 0, max = 2) 
  sample_exponential <- rexp(30, rate = 1)  
  
  t_test_chi_sq <- t.test(sample_chi_sq, mu = true_mean_chi_sq, alternative = "two.sided")
  t_test_uniform <- t.test(sample_uniform, mu = true_mean_uniform, alternative = "two.sided")
  t_test_exponential <- t.test(sample_exponential, mu = true_mean_exponential, alternative = "two.sided")
  
  if (t_test_chi_sq$p.value < alpha) {
    rej_chi_sq <- rej_chi_sq + 1
  }
  
  if (t_test_uniform$p.value < alpha) {
    rej_uniform <- rej_uniform + 1
  }
  
  if (t_test_exponential$p.value < alpha) {
    rej_exponential <- rej_exponential + 1
  }
}

t1e_chi_sq <- rej_chi_sq / n
t1e_uniform <- rej_uniform / n
t1e_exponential <- rej_exponential / n

# Print the results
cat("Type I Error Rate for χ²(1) Data:", t1e_chi_sq, "\n")
cat("Type I Error Rate for Uniform(0,2) Data:", t1e_uniform, "\n")
cat("Type I Error Rate for Exponential(1) Data:", t1e_exponential, "\n")

```

# HW4



## Question{}

1. 考虑$m=1000$个假设，其中前95%个原假设成立，后5%个对立假设成立。在原假设下，$p\sim U(0,1)$，在对立假设下，$p\sim Beta(0.1,1)$（可用rbeta生成）。应用Bonferroni矫正方法与BH矫正应用于生成的m个p值，得到矫正后的$p-value$，与$\alpha=0.1$比较确定是否拒绝原假设。基于M=1000次模拟，可估计FWER、FDR、TPR，输出到表格中。

2. Suppose the population has the exponential distribution with
rate $λ$, then the MLE of λ is $\hat{\lambda} = 1/\bar{X}$ , where $\bar{X}$ is the sample mean. It can be derived that the expectation of $\hat{λ}$ is λn/(n − 1),
so that the estimation bias is λ/(n − 1). The standard error $\hat{λ}$ is
λn/[(n − 1)√n − 2]. Conduct a simulation study to verify the
performance of the bootstrap method.
The true value of λ = 2.
The sample size n = 5, 10, 20.
The number of bootstrap replicates B = 1000.
The simulations are repeated for m = 1000 times.
Compare the mean bootstrap bias and bootstrap standard error
with the theoretical ones. Comment on the results.



3. (7.3) Obtain a bootstrap t confidence interval estimate for the correlation statistic
in Example 7.2 (law data in bootstrap)




## Answer
### Question 1{}
```{r}
set.seed(114514)
# bonferroni
m <- 1000
M <- 1000
alpha <- 0.1
FWER <- numeric(M)
FDR <- numeric(M)
TPR <- numeric(M)

for (i in 1:M){
  p1 <- runif(950)
  p2 <- rbeta(50,0.1,1)
  p <- c(p1,p2)
  p <- p.adjust(p,method = 'bonferroni')
  FWER[i] <- sum(p[1:950]<alpha)>0
  FDR[i] <- sum(p[1:950]<alpha)/sum(p<alpha)
  TPR[i] <- sum(p[1:950]>alpha)/(sum(p[1:950]>alpha)+sum(p[951:1000]<alpha))
}
cat("Mean FWER:", mean(FWER), "\n")
cat("Mean FDR:", mean(FDR), "\n")
cat("Mean TPR:", mean(TPR), "\n")
```

```{r}
set.seed(114514)
# BH
m <- 1000
M <- 1000
alpha <- 0.1
FWER <- numeric(M)
FDR <- numeric(M)
TPR <- numeric(M)

for (i in 1:M){
  p1 <- runif(950)
  p2 <- rbeta(50,0.1,1)
  p <- c(p1,p2)
  p <- p.adjust(p,method = 'BH')
  FWER[i] <- sum(p[1:950]<alpha)>0
  FDR[i] <- sum(p[1:950]<alpha)/sum(p<alpha)
  TPR[i] <- sum(p[1:950]>alpha)/(sum(p[1:950]>alpha)+sum(p[951:1000]<alpha))
}
cat("Mean FWER:", mean(FWER), "\n")
cat("Mean FDR:", mean(FDR), "\n")
cat("Mean TPR:", mean(TPR), "\n")
```

# Correction Results

| Metric            | Bonferroni Correction | BH Correction |
| ----------------- | -------------------- | ------------- |
| Mean FWER         | 0.094                | 0.93          |
| Mean FDR          | 0.004                | 0.093         |
| Mean TPR          | 0.979                | 0.971         |



### Question 2{}


```{r}
#set.seed(114514)
lambda <- 2
n <- c(5,10,20)
B <- 1000
m <- 1000

for (j in n){
  x <- rexp(j,lambda)
  xstar <- numeric(m)
  for (i in 1:m){
    xstar[i] <- 1/mean(sample(x,B,replace = T))
  }
  cat("n=",j,"\nBootstrap method vs theoretical on Bias:",mean(xstar)-1/mean(x),lambda*j/(j-1)-1/mean(x),"\n")
  cat("Bootstrap method vs theoretical on SE:",sd(xstar),lambda*j/((j-1)*sqrt(j-2)),"\n")
}


```

It is obvious that using Bootstrap can reduce both the Bias and standard error, regardless of the sample size in n.

### Question 3{}
```{r}
library(bootstrap)

#set up the bootstrap
B <- 200 #number of replicates
n <- nrow(law) #sample size
R <- numeric(B) #storage for replicates
#bootstrap estimate of standard error of R
r <- cor(law$LSAT,law$GPA)
for (b in 1:B) {
#randomly select the indices
i <- sample(1:n, size = n, replace = TRUE)
LSAT <- law$LSAT[i] #i is a vector of indices
GPA <- law$GPA[i]
R[b] <- cor(LSAT, GPA)
}
#output
t <- (R-r)/sd(R)
cat("Bootstrap t Confidence Interval for Correlation:",r+quantile(t, c(0.025, 0.975))*sd(R))
```


# HW5


## Question{}

1. (7.5) Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the mean time between failures $1/λ$ by the standard normal, basic, percentile,
and BCa methods. Compare the intervals and explain why they may differ.。

2. (7.8).Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $θ$



3. (7.11) In Example 7.18, leave-one-out (n-fold) cross validation was used to select the
best fitting model. Use leave-two-out cross validation to compare the models. 




## Answer
### Question 1{}


Recall the four methods first.\

+ standard normal:$(\hat\theta-z_{1-\alpha/2}\hat{se}(\hat\theta),\hat\theta-z_{\alpha/2}\hat{se}(\hat\theta))$
+ basic:$(2\hat\theta-\hat\theta^*_{1-\alpha/2},2\hat\theta-\hat\theta^*_{\alpha/2})$
+ percentile:$(\hat\theta^*_{\alpha/2},\hat\theta^*_{1-\alpha/2})$
+ BCa:$(\hat\theta_{\alpha_1}^*,\hat\theta_{\alpha_2}^*)$
$$\alpha_1=\phi(\hat z_0+\frac{\hat z_0+z_{\alpha/2}}{1-\hat a(\hat z_0+z_{\alpha/2})}),\alpha_2=\phi(\hat z_0+\frac{\hat z_0+z_{1-\alpha/2}}{1-\hat a(\hat z_0+z_{1-\alpha/2})})$$
$$\hat z_0=\phi^{-1}(\frac{1}{B}\sum_{i=1}^n I(\hat\theta^{(b)}<\hat\theta)),\hat a=\frac{\sum_{i=1}^n(\bar\theta-\theta_i)^3}{6(\sum_{i=1}^n(\bar\theta-\theta_i)^2)^{3/2}}$$
We can use the **boot** and **boot.ci** functions to construct confidence intervals.




The comparison reveals that both the basic and percentile methods yield intervals of similar length, closely approximating the normal method. In contrast, the BCa method produces longer intervals than the other three. The basic method's confidence interval leans more toward the left, while the confidence intervals of the normal, percentile, and BCa methods shift towards the right. This suggests that both the upper and lower bounds have increased. 


### Question 2{}

We use "leave-one-out" to obtain $\hat\Sigma_{(i)}$ and compute $\hat\theta_{(i)}$. Notice that the MLE of $\Sigma$ is $$\hat\Sigma=\frac{1}{n}\sum_{i=1}^n(X_{(i)}-\bar X)(X_{(i)}-\bar X)^T$$, so we need to slightly modify the cov(X).

```{r}
data("scor",package="bootstrap")
data1 = scor
n = nrow(data1)
sigma.hat = matrix(0,5,5)
lambda.hat = numeric(5)
theta.jack = numeric(n)
for(i in 1:n){
  data_jack = data1[-i,]  # leave-one-out
  sigma.hat = (n-2)*cov(data_jack)/(n-1) # MLE of Sigma
  lambda.hat = eigen(sigma.hat)$values
  theta.jack[i] = lambda.hat[1]/sum(lambda.hat)
}
```

```{r}
sigma = (n-1)*cov(data1)/n
lambda.hat = eigen(sigma)$values
theta.hat = lambda.hat[1]/sum(lambda.hat)
bias.jack = (n-1)*(mean(theta.jack)-theta.hat)
se.jack = sqrt((n-1)*mean((theta.jack-mean(theta.hat))^2))
round(c(bias.jack=bias.jack,se.jack=se.jack),3)
```



### Question 3{}

To perform leave-two-out cross-validation, follow these steps:

1. Iterate through $i=1,2,\dots,n$ and $j=1,2,\dots,n$ with the constraint $i\neq j$. Take $(x_i,y_i)$ and $(x_j,y_j)$ as the test points while using the remaining observations to fit the model. Calculate the prediction errors as follows: $e_{ij1}=y_i-\hat y_i$ and $e_{ij2}=y_j-\hat y_j$.

2. Estimate the mean of the squared prediction errors using the formula: $\displaystyle\hat\sigma_{\epsilon}^2=\frac{2}{n(n-1)}\sum_{i=1}^{n-1}\sum_{j=i+1}^n e_{ij}^2$, where $e_{ij}^2$ is defined as $\frac{e_{ij1}^2+e_{ij2}^2}{2}$.

Note that the matrix $e_k$ contains $n(n+1)/2$ zeros, so it's necessary to make a slight adjustment in the mean calculation when computing it.

```{r}
library(DAAG); attach(ironslag)
n <- length(magnetic) 
e1 <- e2 <- e3 <- e4 <- matrix(0,n,n) # store the squared prediction errors

# fit models on leave-two-out samples
for (i in 1:(n-1)){
  for (j in (i+1):n){
    y <- magnetic[-c(i,j)]
    x <- chemical[-c(i,j)]
    
    J1 <- lm(y ~ x)
    yhat1 <- J1$coef[1] + J1$coef[2]*chemical[c(i,j)]
    e1[i,j] <- mean((magnetic[c(i,j)] - yhat1)^2)
  
    J2 <- lm(y ~ x + I(x^2))
    yhat2 <- J2$coef[1]+J2$coef[2]*chemical[c(i,j)]+J2$coef[3]*chemical[c(i,j)]^2
    e2[i,j] <- mean((magnetic[c(i,j)] - yhat2)^2)
  
    J3 <- lm(log(y) ~ x)
    logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[c(i,j)]
    yhat3 <- exp(logyhat3)
    e3[i,j] <- mean((magnetic[c(i,j)] - yhat3)^2)
    
    J4 <- lm(log(y) ~ log(x))
    logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[c(i,j)])
    yhat4 <- exp(logyhat4)
    e4[i,j] <- mean((magnetic[c(i,j)] - yhat4)^2)
  }
}
```

```{r}
2*n/(n-1)*c(mean(e1), mean(e2), mean(e3), mean(e4))
```

Based on the prediction error criterion, Model 2, which is the quadratic model, appears to be the most suitable fit for the data. This outcome aligns with the findings of the leave-one-out method.


# HW6


## Question{}

1. Proof the Stationarity of Metropolis-Hastings sampler Algorithm in continuous situation

2. (8.1).Implement the two-sample Cram´er-von Mises test for equal distributions as a permutation test. Apply the test to the data in Examples 8.1 and 8.2

3. (8.3) The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion
is not applicable for unequal sample sizes. Implement a permutation test for
equal variance based on the maximum number of extreme points that applies
when sample sizes are not necessarily equal




## Answer
### Question 1{}

要证$\pi_ip_{ij}=\pi_jp_{ji}$，首先证明$p_{ij}=a_{ij}q_{ij}$。

$i\neq j$时，$$p_{ij}=P(X_{n+1}=j|X_n=i)P(\alpha_{ij}\leq u)+P(X_{n+1}=j|X_n=i)P(u<\alpha_{ij})$$
$$=P(X_{n+1}=j|X_n=i)P(u<\alpha_{ij})=q_{ij}a_{ij}$$
$i = j$时，$$p_{ij}=P(X_{n+1}=j|X_n=i)P(\alpha_{ij}\leq u)+P(X_{n+1}=j|X_n=i)P(u<\alpha_{ij})$$
$$=P(\alpha_{ij}\leq u)+P(X_{n+1}=j|X_n=i)P(u<\alpha_{ij})=1-a_{ij}+q_{ij}a_{ij}=q_{ij}a_{ij}$$
因此，
$$\pi_ip_{ij}=\pi_iq_{ij}a_{ij}=\pi_iq_{ij}min(1,\frac{\pi_jq_{ji}}{\pi_iq_{ij}})$$
$$=min(\pi_jq_{ji},\pi_iq_{ij})=\pi_jq_{ji}min(1,\frac{\pi_iq_{ij}}{\pi_jq_{ji}})=\pi_jp_{ji}$$


### Question 2{}

* Two-sample tests for univariate data (continued)
    + Cramer-von Mises statistic
    $$
    W_2=\frac{mn}{(m+n)^2}\left[
    \sum_{i=1}^n(F_n(x_i)-G_m(x_i))^2+\sum_{j=1}^m(F_n(y_j)-G_m(y_j))^2\right]
    $$
    $F_n$: the ecdf of the sample $x_1,\ldots,x_n$; $G_m$: the ecdf of the sample $y_1,\ldots,y_m$.

```{r}
attach(chickwts)
x <- sort(as.vector(weight[feed == "soybean"]))
y <- sort(as.vector(weight[feed == "linseed"]))
detach(chickwts)
```




### Question 3{}

```{r}
count5test <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
# return 1 (reject) or 0 (do not reject H0)
return(as.integer(max(c(outx, outy)) > 5))
}

n1 <- 20
n2 <- 30
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)

R <- 999 #number of replicates
z <- c(x, y) #pooled sample
K <- 1:50
n <- length(x)
m <- length(y)
reps <- numeric(R) #storage for replicates
for (i in 1:R) {
#generate indices k for the first sample
k <- sample(K, size = n, replace = FALSE)
x1 <- z[k]
y1 <- z[-k] #complement of x1
reps[i] <- count5test(x1,y1)
}
mean(reps)
```


# HW7


## Question{}

1. 
Consider a model $P(Y = 1 | X1, X2, X3) = \frac{exp(a+b_1X_1+b_2X_2+b_3X_3)}{1+exp(a+b_1X_1+b_2X_2+b_3X_3)}$, where X1 ∼ P(1), X2 ∼ Exp(1) and X3 ∼ B(1, 0.5).
• Design a function that takes as input values N, b1, b2, b3 and f0, and produces the output a.
• Call this function, input values are N = 106, b1 = 0, b2 = 1, b3 = −1, f0 = 0.1, 0.01, 0.001, 0.0001.
• Plot − log f0 vs a.


2. (9.4).Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.



3. (9.7) Implement a Gibbs sampler to generate a bivariate normal chain $(X_t, Y_t)$
with zero means, unit standard deviations, and correlation 0.9. Plot the generated sample after discarding a suitable burn-in sample. Fit a simple linear regression model $Y = \beta_0 + \beta_1X$ to the sample and check the residuals of the model for normality and constant variance. 

4. (9.10) Refer to Example 9.1. Use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until the chain has converged approximately to the target distribution according to $\hat{R} < 1.2$. (See Exercise 9.9.) Also use the coda [212] package to check for convergence of the chain by the Gelman-Rubin
method. Hints: See the help topics for the coda functions gelman.diag,
gelman.plot, as.mcmc, and mcmc.list.




## Answer
### Question 1{}

```{r}
fun <- function(N,b1,b2,b3,f0){
  x1 <- rpois(N,1)
  x2 <- rexp(N,1)
  x3 <- rbinom(N,1,0.5)
  g <- function(alpha){
    tmp <- exp(-alpha-b1*x1-b2*x2-b3*x3); p <- 1/(1+tmp)
    mean(p) - f0
  }
  solution <- uniroot(g,c(-20,0))
  solution$root
}
f0 <- c(0.1,0.01,0.001,0.0001)
a <- c()
for (i in 1:4){
  a[i] <- fun(1e6,0,1,-1,f0[i])
}
plot(-log(f0),a,type = "b")
```


### Question 2{}

The density function of Laplace distribution is $f(x)=\frac{1}{2}e^{-|x|}$, so $r(x_t,y)=\frac{f(Y)}{f(X_t)}=e^{|y|-|x|}$.

```{r}
f <- function(x){
  return(exp(-abs(x))/2)
}
invF <- function(x){
  return(-log(2*(1-x)))
}

rw.Laplace <- function(sigma, x0, N){
  x = numeric(N)
  x[1] = x0
  u = runif(N)
  k = 0
  for (i in 2:N) {
    y = rnorm(1, x[i-1], sigma)
    if (u[i] <= (f(y)/f(x[i-1])))
    x[i] = y else {
    x[i] = x[i-1]
    k = k + 1
  }
}
return(list(x=x, k=k))
}

Gelman.Rubin <- function(psi) {
# psi[i,j] is the statistic psi(X[i,1:j])
# for chain in i-th row of X
  psi = as.matrix(psi)
  n = ncol(psi)
  k = nrow(psi)
  psi.means = rowMeans(psi) #row means
  B = n * var(psi.means) #between variance est.
  psi.w = apply(psi, 1, "var") #within variances
  W = mean(psi.w) #within est.
  v.hat = W*(n-1)/n + (B/n) #upper variance est.
  r.hat = v.hat / W #G-R statistic
  return(r.hat)
}
```

```{r}
set.seed(1234)
n = 12000 #length of chains
k = 4 #number of chains to generate
x0 = c(-10,-5,5,10) #different initial values
sigma = c(0.5, 2, 5) #different variance
b = 1000 #burn-in length
X1 <- X2 <- X3 <- matrix(0,k,n)
rej = matrix(0,3,4)
refline = c(-invF(0.975), invF(0.975))

for(i in 1:k){
  rw1 = rw.Laplace(sigma[1], x0[i], n)
  rw2 = rw.Laplace(sigma[2], x0[i], n)
  rw3 = rw.Laplace(sigma[3], x0[i], n)
  X1[i,]=rw1$x;X2[i,]=rw2$x;X3[i,]=rw3$x
  rej[,i] = c(rw1$k,rw2$k,rw3$k)
}

# compute diagnostic statistics
psi1 <- t(apply(X1, 1, cumsum))
psi2 <- t(apply(X2, 1, cumsum))
psi3 <- t(apply(X3, 1, cumsum))

for (i in 1:k){
  psi1[i,] = psi1[i,] / (1:ncol(psi1))
  psi2[i,] = psi2[i,] / (1:ncol(psi2))
  psi3[i,] = psi3[i,] / (1:ncol(psi3))
}
print(c(Gelman.Rubin(psi1),Gelman.Rubin(psi2),Gelman.Rubin(psi3)))
```
We choose the first chain of each variance and plot the generated samples. Also, we plot the $\hat R$ over time 1001 to 12000 to monitor convergence.



```{r}
1-apply(rej, 1, mean)/n
```

From the results, the value of $\hat R$ is below 1.2 within approximately 9000, 3000, 1500 iterations for $\sigma=0.5,2,5$, respectively, which implies that the chain has converged to the target distribution. It's evident that as the variance become larger, the chain will converge faster but the acceptance rate will become smaller.

### Question 3{}

First we define a function to use Gibbs sampler to generate a bivariate normal chain $(X_t,Y_t)$. 

```{r}
Gibbs.binorm <- function(N,mu1=0,mu2=0,sigma1=1,sigma2=1,rho=0.9,x0=c(mu1,mu2)){
  X = matrix(0, N, 2) #the chain, a bivariate sample
  s1 = sqrt(1-rho^2)*sigma1
  s2 = sqrt(1-rho^2)*sigma2
  X[1,] = x0
  for (i in 2:N) {
    y1 = X[i-1, 2]
    m1 = mu1 + rho * (y1 - mu2) * sigma1/sigma2
    X[i, 1] = rnorm(1, m1, s1)
    x1 = X[i, 1]
    m2 = mu2 + rho * (x1 - mu1) * sigma2/sigma1
    X[i, 2] = rnorm(1, m2, s2)
  }
  return(X)
}
```

To monitor the convergence of the chain, we set the scalar summary statistic $\psi_{ij}$ as the mean of the $i^{th}$ chain up to time $j$ and do the process separately for $X_t$ and $Y_t$.

```{r}
set.seed(1234)
k = 4 #number of chains to generate
N = 10000 #length of chain
X <- matrix(0,N,2*k)
Xt <- Yt <- matrix(0,k,N)
x0 = matrix(rep(c(-1,-0.5,0.5,1),2),4) #initial values
psix = psiy = matrix(0,k,N)

for(i in 1:k){
  X[,(2*i-1):(2*i)] = Gibbs.binorm(N,x0=x0[i,])
  Xt[i,] = t(X[,2*i-1])
  Yt[i,] = t(X[,2*i])
}
psix = t(apply(Xt, 1, cumsum))
psiy = t(apply(Yt, 1, cumsum))
for (i in 1:k){
  psix[i,] = psix[i,] / (1:ncol(psix))
  psiy[i,] = psiy[i,] / (1:ncol(psiy))
}
print(c(Gelman.Rubin(psix),Gelman.Rubin(psiy)))
```

```{r,out.width='50%', out.height='50%'}
b = 1000 #burn-in length
rhatx = rhaty = rep(0,N)
for (j in (b+1):N){
  rhatx[j] <- Gelman.Rubin(psix[,1:j])
  rhaty[j] <- Gelman.Rubin(psiy[,1:j])
}

```

The figures of $\hat R$ shows that the chain converges approximately within 2000 iterations. Next, we use the first generated chain to show the samples and other chains' figure is similar to it. The generated samples are shown as below. The scatter plot seems like an ellipse which conforms to a bivariate normal distribution.

```{r}
x <- X[(b+1):N, 1:2]
plot(x, main="", cex=.5, xlab=bquote(X[t]),
ylab=bquote(Y[t]), ylim=range(x[,2]))
```

```{r}
fit = lm(x[,2] ~ x[,1]) #linear model Y=a+bX
summary(fit)
```

```{r,out.width='50%', out.height='50%'}
plot(fit)
```

From the QQ plot we can see that the residuals of the model approximately follow a normal distribution. Besides, from the Scale-Location plot we can see that the trend line is nearly horizontal so the residuals have constant variance.

### Question 4{}

```{r}
f <- function(x, sigma) {
if (any(x < 0)) return (0)
stopifnot(sigma > 0)
return((x / sigma^2) * exp(-x^2 / (2*sigma^2)))
}

m <- 10000
sigma <- 4
x <- numeric(m)
x[1] <- rchisq(1, df=1)
k <- 0
u <- runif(m)
for (i in 2:m) {
xt <- x[i-1]
y <- rchisq(1, df = xt)
num <- f(y, sigma) * dchisq(xt, df = y)
den <- f(xt, sigma) * dchisq(y, df = xt)
if (u[i] <= num/den) x[i] <- y else {
x[i] <- xt
k <- k+1 #y is rejected
}
}

index <- 5000:5500
y1 <- x[index]
plot(index, y1, type="l", main="", ylab="x")
```

```{r}
x0 <- x
x <- numeric(m)
x[1] <- rchisq(1, df=1)
k <- 0
u <- runif(m)
for (i in 2:m) {
xt <- x[i-1]
y <- rchisq(1, df = xt)
num <- f(y, sigma) * dchisq(xt, df = y)
den <- f(xt, sigma) * dchisq(y, df = xt)
if (u[i] <= num/den) x[i] <- y else {
x[i] <- xt
k <- k+1 #y is rejected
}
}

index <- 5000:5500
y1 <- x[index]
plot(index, y1, type="l", main="", ylab="x")
```


```{r}
library(coda)
summary(as.mcmc(x))
```
```{r}
mcmc_obj <- mcmc.list(mcmc(x0),mcmc(x))

# 查看 summary
summary(mcmc_obj)
gelman.diag(mcmc_obj)
gelman.plot(mcmc_obj)
```

# HW8


## Question{}

1. $X_1,...,X_n\sim Exp(\lambda),u_i\leq X_i\leq v_i$，(1)Solve $\lambda_{MLE}$ by maximizing the likelihood of the observed data directly and EM algorithm. Prove the EM method's linear convergence.
(2)Let $(u_i,v_i)$ be c(11,12,8,9,27,28,13,14,16,17,0,1,23,24,10,11,24,25,2,3).
Get the numeric solution of both methods.

2. (11.8).In the Morra game, the set of optimal strategies are not changed if a constant is subtracted from every entry of the payoff matrix, or a positive constant is multiplied times every entry of the payoff matrix. However, the simplex algorithm may terminate at a different basic feasible point (also optimal).
Compute B <- A + 2, find the solution of game B, and verify that it is one
of the extreme points (11.12)–(11.15) of the original game A. Also find the
value of game A and game B.







## Answer
### Question 1{}

**Directly Optimize**

The likelihood function for the observed data is 
$$L(\lambda)=\prod_{i=1}^nP(u_i\leq X_i\leq v_i)=\prod_{i=1}^n(e^{-\lambda u_i}-e^{-\lambda v_i}),$$ then we have $$logL=\sum_{i=1}^nlog(e^{-\lambda u_i}-e^{-\lambda v_i}),$$ $$\frac{\partial logL}{\partial\lambda}=-\lambda\sum_{i=1}^n \frac{u_ie^{-\lambda u_i}-v_ie^{-\lambda v_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}.$$ We can obtain the MLE $\hat\lambda$ by solving the equation $\frac{\partial logL}{\partial\lambda}=0$. Codes for achieving the process are as below(here we can use optimize() to maximize $logL$ directly).

```{r}
data = c(11,12,8,9,27,28,13,14,16,17,0,1,23,24,10,11,24,25,2,3)
X = matrix(data, nrow=10, byrow=TRUE)
u = X[,1]; v = X[,2]
logL <- function(lambda){
  s = sum(log(exp(-lambda*u)-exp(-lambda*v)))
  return(s)
}
res = optimize(logL, lower=0, upper=5, maximum=TRUE)
lambdahat = res[[1]]
```

**EM algorithm**

1.Initiate $\lambda$ as $\lambda_0$ \
2.E-step: When the interval is given, the conditional expectation is:
$$E(X|u<X\leq v)=\int_u^v\frac{\lambda_0 xe^{-\lambda_0 x}}{e^{-\lambda_0 u}-e^{-\lambda_0 v}}dx
=\frac{ue^{-\lambda_0 u}-ve^{-\lambda_0 v}}{e^{-\lambda_0 u}-e^{-\lambda_0 v}}+\frac{1}{\lambda_0}.$$
3.M-step: the log-likelihood is
$$logL=\frac{n}{\lambda}-\lambda\sum_{i=1}^n(\frac{u_ie^{-\lambda_0 u_i}-v_ie^{-\lambda_0 v_i}}{e^{-\lambda_0 u_i}-e^{-\lambda_0 v_i}}+\frac{1}{\lambda_0}),$$ so by maximizing $logL$, we can obtain $$\lambda_1=\frac{n}{\sum_{i=1}^n\frac{u_ie^{-\lambda_0 u_i}-v_ie^{-\lambda_0 v_i}}{e^{-\lambda_0 u_i}-e^{-\lambda_0 v_i}}+\frac{n}{\lambda_0}}.$$
4.In steps 2 and 3, replace $\lambda_0$ with $\lambda_1$, and repeat this process
until convergence (iteration).

Denote $f(\lambda)=\frac{n}{\sum_{i=1}^n\frac{u_ie^{-\lambda u_i}-v_ie^{-\lambda v_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}+\frac{n}{\lambda}}$. We can compute $|f'(\lambda)|<1$ and by the contraction mapping theorem, the EM process converges to a fixed point $\tilde\lambda$ which satisfies $\tilde\lambda=f(\tilde\lambda)$. Specificly, $\tilde\lambda$ satisfies $$\tilde\lambda\sum_{i=1}^n\frac{u_ie^{-\tilde\lambda u_i}-v_ie^{-\tilde\lambda v_i}}{e^{-\tilde\lambda u_i}-e^{-\tilde\lambda v_i}}+n=n.$$ This is the same equation as $\frac{\partial logL}{\partial\lambda}=0$ so we claim that $\hat\lambda=\tilde\lambda$, which means EM algorithm has the same value as optimizing directly. 

```{r}
n = 10
delta = 1
lambda = numeric(100) 
lambda[1] = 1 #initialize 
i = 1
# E-step
Econd <- function(lambda,u,v){ 
  (u*exp(-lambda*u)-v*exp(-lambda*v))/(exp(-lambda*u)-exp(-lambda*v))+1/lambda
}
while(delta > 1e-5){
  lambda[i+1] =  n/sum(Econd(lambda[i],u,v)) # M-step
  delta = abs(lambda[i+1]-lambda[i])
  i = i+1
}
lambdahat.EM = lambda[i]
round(c(lambdahat,lambdahat.EM),5)
```

### Question 2{}

```{r}
A <- matrix(c( 0,-2,-2,3,0,0,4,0,0,
2,0,0,0,-3,-3,4,0,0,
2,0,0,3,0,0,0,-4,-4,
-3,0,-3,0,4,0,0,5,0,
0,3,0,-4,0,-4,0,5,0,
0,3,0,0,4,0,-5,0,-5,
-4,-4,0,0,0,5,0,0,6,
0,0,4,-5,-5,0,0,0,6,
0,0,4,0,0,5,-6,-6,0), 9, 9)
library(boot) #needed for simplex function
B <- A+2
```

```{r}
solve.game <- function(A) {
#solve the two player zero-sum game by simplex method
#optimize for player 1, then player 2
#maximize v subject to ...
#let x strategies 1:m, and put v as extra variable
#A1, the <= constraints
#
min.A <- min(A)
A <- A - min.A #so that v >= 0
max.A <- max(A)
A <- A / max(A)
m <- nrow(A)
n <- ncol(A)
it <- n^3
a <- c(rep(0, m), 1) #objective function
A1 <- -cbind(t(A), rep(-1, n)) #constraints <=
b1 <- rep(0, n)
A3 <- t(as.matrix(c(rep(1, m), 0))) #constraints sum(x)=1
b3 <- 1
sx <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
maxi=TRUE, n.iter=it)
#the ’solution’ is [x1,x2,...,xm | value of game]
#
#minimize v subject to ...
#let y strategies 1:n, with v as extra variable
a <- c(rep(0, n), 1) #objective function
A1 <- cbind(A, rep(-1, m)) #constraints <=
b1 <- rep(0, m)
A3 <- t(as.matrix(c(rep(1, n), 0))) #constraints sum(y)=1
b3 <- 1
sy <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
maxi=FALSE, n.iter=it)
soln <- list("A" = A * max.A + min.A,
"x" = sx$soln[1:m],
"y" = sy$soln[1:n],
"v" = sx$soln[m+1] * max.A + min.A)
soln
}

s1 <- solve.game(A)
s2 <- solve.game(B)
round(cbind(s1$x, s2$x), 7)
round(cbind(s1$v, s2$v), 7)
```

It is obvious that both payoff matrix lead to the same optimal strategy in game A and game B. But the values of both games are different.  From the property of simplex method in LP, the solution must be one of the extreme points.


# HW9


## 2.1.3

**Q4:**Why do you need to use unlist() to convert a list to an atomic vector? Why doesn’t as.vector() work?

**A:**Note that list is also a vector but it is not a atomic vector. Therefore, as.vector() doesn't work for list. We use unlist() to get rid of the nested structure.


## 2.3.1

**Q1:**What does dim() return when applied to a vector?

**A:**dim() will return NULL when applied to a 1d vector.

**Q2:**If is.matrix(x) is TRUE, what will is.array(x) return?

**A:**is.array(x) will return TRUE because a matrix is also a two-dimensional array.

## 2.4.5

**Q2:**What does as.matrix() do when applied to a data frame with columns of different types?

**A:**From help documents of as.matrix(): The method for data frames will return a character matrix if there is only atomic columns and any non-(numeric/logical/complex) column, applying as.vector to factors and format to other non-character columns. Otherwise, the usual coercion hierarchy (logical < integer < double < complex) will be used.

**Q3:**Can you have a data frame with 0 rows? What about 0 columns?

**A:**Yes, here is an example:
```{r}
x = data.frame(c())
nrow(x);ncol(x)
```

## Exercise 11.1.2

lapply() can be used to apply the function to every colume of a data frame. This function needs a numeric input, so if the colume isn't numeric, we can return it directly. 

```{r}
scale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
head(data.frame(lapply(iris,function(x) if(is.numeric(x)) scale01(x) else x)))
```
If we only want to apply the function to numeric columes, we can use sapply() first to extract the numeric columes.

```{r}
head(data.frame(lapply(iris[sapply(iris,is.numeric)],scale01)))
```

## Exercise 11.2.5 

For a numeric data frame:

```{r}
vapply(cars, sd, numeric(1))
```

For a mixed data frame:

```{r}
vapply(iris[vapply(iris, is.numeric, logical(1))],
       sd, 
       numeric(1))
```

## Exercise 9.8
This example appears in [40]. Consider the bivariate density
f(x, y),It can be shown (see e.g. [23]) that for fixed a, b, n, the conditional distributions are Binomial(n, y) and Beta(x+ a, n − x+ b). Use the Gibbs sampler to
generate a chain with target joint density f(x, y).


```{r}
library(microbenchmark)
library(Rcpp)

sourceCpp("../src/GibbsC.cpp")

n <- 10
a <- 2
b <- 3
num_samples <- 1000

gibbs_sampler_r <- function(num_samples, n, a, b) {
  samples <- matrix(0, nrow = num_samples, ncol = 2)
  
  x <- 0
  y <- 0.5
  
  for (i in 1:num_samples) {
    x <- rbinom(1, n, y)
    y <- rbeta(1, x + a, n - x + b)
    
    samples[i, ] <- c(x, y)
  }
  
  return(samples)
}

mb <- microbenchmark(
  gibbs_sampler_r(num_samples, n, a, b),
  gibbsSampler(num_samples, n, a, b),
  times = 100
)

summary(mb)[,c(1,3,5,6)]

```

