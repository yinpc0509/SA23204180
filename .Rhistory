theta <- matrix(0, nrow = p, ncol = k)
} else {
theta <- theta_init
}
for (i in 1:max_iter) {
scores_mat <- features_mat %*% theta
exp_scores_mat <- exp(scores_mat)
exp_scores_mat_div_sum_exp <- exp_scores_mat / rowSums(exp_scores_mat, na.rm = TRUE)
temp_mat <- (exp_scores_mat_div_sum_exp - exp_scores_mat_div_sum_exp^2) * rewards_mat
temp <- array(0, dim = c(n, p, k))
for (i in 1:n){
for (j in 1:p){
for (m in 1:k){
temp[i,j,k]=array(temp_mat, dim = c(n, 1, k))[i,1,m] * array(features_mat, dim = c(n, p, 1))[i,j,1]
}
}
}
grad_theta_mat <- apply(temp, c(1, 2), mean)
theta <- theta + (learning_rate) * (grad_theta_mat)
theta <- sweep(theta, 2, theta[, 1], FUN = "-")
}
return(theta)
}
# Example Usage for convex_relaxation:
set.seed(123)
features_mat <- matrix(rnorm(250), ncol = 5)
beta <- matrix(rnorm(10), ncol = 2)
rewards_mat <- features_mat %*% beta+matrix(runif(100), ncol = 2)
theta_result_convex <- convex_relaxation(features_mat, rewards_mat)
print("Theta Result (Convex Relaxation):")
print(theta_result_convex)
theta_result_logistic <- logistic_policy(features_mat, rewards_mat)
- reg_param %*% theta
grad_theta_mat
n <- nrow(features_mat)
p <- ncol(features_mat)
k <- ncol(rewards_mat)
if (is.null(theta_init)) {
theta <- matrix(0, nrow = p, ncol = k)
} else {
theta <- theta_init
}
n <- nrow(features_mat)
p <- ncol(features_mat)
k <- ncol(rewards_mat)
theta <- matrix(0, nrow = p, ncol = k)
for (i in 1:max_iter) {
scores_mat <- features_mat %*% theta
exp_scores_mat <- exp(scores_mat)
exp_scores_mat_div_sum_exp <- exp_scores_mat / rowSums(exp_scores_mat, na.rm = TRUE)
temp_mat <- (exp_scores_mat_div_sum_exp - exp_scores_mat_div_sum_exp^2) * rewards_mat
temp <- array(0, dim = c(n, p, k))
for (i in 1:n){
for (j in 1:p){
for (m in 1:k){
temp[i,j,k]=array(temp_mat, dim = c(n, 1, k))[i,1,m] * array(features_mat, dim = c(n, p, 1))[i,j,1]
}
}
}
grad_theta_mat <- apply(temp, c(1, 2), mean)
theta <- theta + (learning_rate) * (grad_theta_mat)
theta <- sweep(theta, 2, theta[, 1], FUN = "-")
}
n <- nrow(features_mat)
p <- ncol(features_mat)
k <- ncol(rewards_mat)
theta <- matrix(0, nrow = p, ncol = k)
for (i in 1:100) {
scores_mat <- features_mat %*% theta
exp_scores_mat <- exp(scores_mat)
exp_scores_mat_div_sum_exp <- exp_scores_mat / rowSums(exp_scores_mat, na.rm = TRUE)
temp_mat <- (exp_scores_mat_div_sum_exp - exp_scores_mat_div_sum_exp^2) * rewards_mat
temp <- array(0, dim = c(n, p, k))
for (i in 1:n){
for (j in 1:p){
for (m in 1:k){
temp[i,j,k]=array(temp_mat, dim = c(n, 1, k))[i,1,m] * array(features_mat, dim = c(n, p, 1))[i,j,1]
}
}
}
grad_theta_mat <- apply(temp, c(1, 2), mean)
theta <- theta + (learning_rate) * (grad_theta_mat)
theta <- sweep(theta, 2, theta[, 1], FUN = "-")
}
n <- nrow(features_mat)
p <- ncol(features_mat)
k <- ncol(rewards_mat)
theta <- matrix(0, nrow = p, ncol = k)
for (i in 1:100) {
scores_mat <- features_mat %*% theta
exp_scores_mat <- exp(scores_mat)
exp_scores_mat_div_sum_exp <- exp_scores_mat / rowSums(exp_scores_mat, na.rm = TRUE)
temp_mat <- (exp_scores_mat_div_sum_exp - exp_scores_mat_div_sum_exp^2) * rewards_mat
temp <- array(0, dim = c(n, p, k))
for (i in 1:n){
for (j in 1:p){
for (m in 1:k){
temp[i,j,k]=array(temp_mat, dim = c(n, 1, k))[i,1,m] * array(features_mat, dim = c(n, p, 1))[i,j,1]
}
}
}
grad_theta_mat <- apply(temp, c(1, 2), mean)
theta <- theta + (0.01) * (grad_theta_mat)
theta <- sweep(theta, 2, theta[, 1], FUN = "-")
}
grad_theta_mat
dim(grad_theta_mat)
0.1*grad_theta_mat
theta
dim(temp)
dim(grad_theta_mat)
n <- nrow(features_mat)
p <- ncol(features_mat)
k <- ncol(rewards_mat)
theta <- matrix(0, nrow = p, ncol = k)
for (i in 1:100) {
scores_mat <- features_mat %*% theta
exp_scores_mat <- exp(scores_mat)
exp_scores_mat_div_sum_exp <- exp_scores_mat / rowSums(exp_scores_mat, na.rm = TRUE)
temp_mat <- (exp_scores_mat_div_sum_exp - exp_scores_mat_div_sum_exp^2) * rewards_mat
temp <- array(0, dim = c(n, p, k))
for (i in 1:n){
for (j in 1:p){
for (m in 1:k){
temp[i,j,k]=array(temp_mat, dim = c(n, 1, k))[i,1,m] * array(features_mat, dim = c(n, p, 1))[i,j,1]
}
}
}
grad_theta_mat <- apply(temp, c(2, 3), mean)
theta <- theta + (0.01) * (grad_theta_mat)
theta <- sweep(theta[, 2], theta[, 1], FUN = "-")
}
theta[, 2]
theta[, 1]
sweep(theta[, 2], theta[, 1], FUN = "-")
n <- nrow(features_mat)
p <- ncol(features_mat)
k <- ncol(rewards_mat)
theta <- matrix(0, nrow = p, ncol = k)
for (i in 1:100) {
scores_mat <- features_mat %*% theta
exp_scores_mat <- exp(scores_mat)
exp_scores_mat_div_sum_exp <- exp_scores_mat / rowSums(exp_scores_mat, na.rm = TRUE)
temp_mat <- (exp_scores_mat_div_sum_exp - exp_scores_mat_div_sum_exp^2) * rewards_mat
temp <- array(0, dim = c(n, p, k))
for (i in 1:n){
for (j in 1:p){
for (m in 1:k){
temp[i,j,k]=array(temp_mat, dim = c(n, 1, k))[i,1,m] * array(features_mat, dim = c(n, p, 1))[i,j,1]
}
}
}
grad_theta_mat <- apply(temp, c(2, 3), mean)
theta <- theta + (0.01) * (grad_theta_mat)
theta <- sweep(theta, MARGIN = 2, theta[, 1], "-")
}
logistic_policy <- function(features_mat, rewards_mat, max_iter = 1000,
learning_rate = 1, reg_param = 0, theta_init = NULL, intercept = TRUE) {
n <- nrow(features_mat)
p <- ncol(features_mat)
k <- ncol(rewards_mat)
if (is.null(theta_init)) {
theta <- matrix(0, nrow = p, ncol = k)
} else {
theta <- theta_init
}
for (i in 1:max_iter) {
scores_mat <- features_mat %*% theta
exp_scores_mat <- exp(scores_mat)
exp_scores_mat_div_sum_exp <- exp_scores_mat / rowSums(exp_scores_mat, na.rm = TRUE)
temp_mat <- (exp_scores_mat_div_sum_exp - exp_scores_mat_div_sum_exp^2) * rewards_mat
temp <- array(0, dim = c(n, p, k))
for (i in 1:n){
for (j in 1:p){
for (m in 1:k){
temp[i,j,k]=array(temp_mat, dim = c(n, 1, k))[i,1,m] * array(features_mat, dim = c(n, p, 1))[i,j,1]
}
}
}
grad_theta_mat <- apply(temp, c(1, 2), mean)
theta <- theta + (learning_rate) * (grad_theta_mat)
theta <- sweep(theta, MARGIN = 2, theta[, 1], "-")
}
return(theta)
}
# Example Usage for convex_relaxation:
set.seed(123)
features_mat <- matrix(rnorm(250), ncol = 5)
beta <- matrix(rnorm(10), ncol = 2)
rewards_mat <- features_mat %*% beta+matrix(runif(100), ncol = 2)
theta_result_convex <- convex_relaxation(features_mat, rewards_mat)
print("Theta Result (Convex Relaxation):")
print(theta_result_convex)
theta_result_logistic <- logistic_policy(features_mat, rewards_mat)
grad_theta_mat
logistic_policy <- function(features_mat, rewards_mat, max_iter = 1000,
learning_rate = 1, reg_param = 0, theta_init = NULL, intercept = TRUE) {
n <- nrow(features_mat)
p <- ncol(features_mat)
k <- ncol(rewards_mat)
if (is.null(theta_init)) {
theta <- matrix(0, nrow = p, ncol = k)
} else {
theta <- theta_init
}
for (i in 1:max_iter) {
scores_mat <- features_mat %*% theta
exp_scores_mat <- exp(scores_mat)
exp_scores_mat_div_sum_exp <- exp_scores_mat / rowSums(exp_scores_mat, na.rm = TRUE)
temp_mat <- (exp_scores_mat_div_sum_exp - exp_scores_mat_div_sum_exp^2) * rewards_mat
temp <- array(0, dim = c(n, p, k))
for (i in 1:n){
for (j in 1:p){
for (m in 1:k){
temp[i,j,k]=array(temp_mat, dim = c(n, 1, k))[i,1,m] * array(features_mat, dim = c(n, p, 1))[i,j,1]
}
}
}
grad_theta_mat <- apply(temp, c(2, 3), mean)
theta <- theta + (learning_rate) * (grad_theta_mat)
theta <- sweep(theta, MARGIN = 2, theta[, 1], "-")
}
return(theta)
}
# Example Usage for convex_relaxation:
set.seed(123)
features_mat <- matrix(rnorm(250), ncol = 5)
beta <- matrix(rnorm(10), ncol = 2)
rewards_mat <- features_mat %*% beta+matrix(runif(100), ncol = 2)
theta_result_convex <- convex_relaxation(features_mat, rewards_mat)
print("Theta Result (Convex Relaxation):")
print(theta_result_convex)
theta_result_logistic <- logistic_policy(features_mat, rewards_mat)
print("Theta Result (theta_result_logistic):")
print(theta_result_logistic)
n <- nrow(features_mat)
p <- ncol(features_mat)
k <- ncol(rewards_mat)
theta <- matrix(0, nrow = p, ncol = k)
for (i in 1:10) {
scores_mat <- features_mat %*% theta
exp_scores_mat <- exp(scores_mat)
exp_scores_mat_div_sum_exp <- exp_scores_mat / rowSums(exp_scores_mat, na.rm = TRUE)
temp_mat <- (exp_scores_mat_div_sum_exp - exp_scores_mat_div_sum_exp^2) * rewards_mat
temp <- array(0, dim = c(n, p, k))
for (i in 1:n){
for (j in 1:p){
for (m in 1:k){
temp[i,j,k]=array(temp_mat, dim = c(n, 1, k))[i,1,m] * array(features_mat, dim = c(n, p, 1))[i,j,1]
}
}
}
grad_theta_mat <- apply(temp, c(2, 3), mean)
theta <- theta + (0.01) * (grad_theta_mat)
theta <- sweep(theta, MARGIN = 2, theta[, 1], "-")
}
theta
t(theta)
t(theta)-theta[,1]
theta[,1]
theta.T
n <- nrow(features_mat)
p <- ncol(features_mat)
k <- ncol(rewards_mat)
theta <- matrix(0, nrow = p, ncol = k)
for (i in 1:10) {
scores_mat <- features_mat %*% theta
exp_scores_mat <- exp(scores_mat)
exp_scores_mat_div_sum_exp <- exp_scores_mat / rowSums(exp_scores_mat, na.rm = TRUE)
temp_mat <- (exp_scores_mat_div_sum_exp - exp_scores_mat_div_sum_exp^2) * rewards_mat
temp <- array(0, dim = c(n, p, k))
for (i in 1:n){
for (j in 1:p){
for (m in 1:k){
temp[i,j,k]=array(temp_mat, dim = c(n, 1, k))[i,1,m] * array(features_mat, dim = c(n, p, 1))[i,j,1]
}
}
}
grad_theta_mat <- apply(temp, c(2, 3), mean)
theta <- theta + (0.01) * (grad_theta_mat)
theta <- sweep(theta, MARGIN = 2, theta[, 1], "-")
theta = t(t(theta)-theta[,1])
}
n <- nrow(features_mat)
p <- ncol(features_mat)
k <- ncol(rewards_mat)
theta <- matrix(0, nrow = p, ncol = k)
for (i in 1:10) {
scores_mat <- features_mat %*% theta
exp_scores_mat <- exp(scores_mat)
exp_scores_mat_div_sum_exp <- exp_scores_mat / rowSums(exp_scores_mat, na.rm = TRUE)
temp_mat <- (exp_scores_mat_div_sum_exp - exp_scores_mat_div_sum_exp^2) * rewards_mat
temp <- array(0, dim = c(n, p, k))
for (i in 1:n){
for (j in 1:p){
for (m in 1:k){
temp[i,j,k]=array(temp_mat, dim = c(n, 1, k))[i,1,m] * array(features_mat, dim = c(n, p, 1))[i,j,1]
}
}
}
grad_theta_mat <- apply(temp, c(2, 3), mean)
theta <- theta + (0.01) * (grad_theta_mat)
theta = t(t(theta)-theta[,1])
}
theta
n <- nrow(features_mat)
p <- ncol(features_mat)
k <- ncol(rewards_mat)
theta <- matrix(1, nrow = p, ncol = k)
for (i in 1:10) {
scores_mat <- features_mat %*% theta
exp_scores_mat <- exp(scores_mat)
exp_scores_mat_div_sum_exp <- exp_scores_mat / rowSums(exp_scores_mat, na.rm = TRUE)
temp_mat <- (exp_scores_mat_div_sum_exp - exp_scores_mat_div_sum_exp^2) * rewards_mat
temp <- array(0, dim = c(n, p, k))
for (i in 1:n){
for (j in 1:p){
for (m in 1:k){
temp[i,j,k]=array(temp_mat, dim = c(n, 1, k))[i,1,m] * array(features_mat, dim = c(n, p, 1))[i,j,1]
}
}
}
grad_theta_mat <- apply(temp, c(2, 3), mean)
theta <- theta + (0.01) * (grad_theta_mat)
theta = t(t(theta)-theta[,1])
}
theta
# Example Usage for convex_relaxation:
set.seed(123)
features_mat <- matrix(rnorm(250), ncol = 5)
beta <- matrix(rnorm(10), ncol = 2)
rewards_mat <- features_mat %*% beta+matrix(runif(100), ncol = 2)
theta_result_convex <- convex_relaxation(features_mat, rewards_mat)
print("Theta Result (Convex Relaxation):")
print(theta_result_convex)
theta_result_logistic <- logistic_policy(features_mat, rewards_mat)
logistic_policy <- function(features_mat, rewards_mat, max_iter = 1000,
learning_rate = 1, reg_param = 0, theta_init = NULL, intercept = TRUE) {
n <- nrow(features_mat)
p <- ncol(features_mat)
k <- ncol(rewards_mat)
if (is.null(theta_init)) {
theta <- matrix(0, nrow = p, ncol = k)
} else {
theta <- theta_init
}
for (i in 1:max_iter) {
scores_mat <- features_mat %*% theta
exp_scores_mat <- exp(scores_mat)
exp_scores_mat_div_sum_exp <- exp_scores_mat / rowSums(exp_scores_mat, na.rm = TRUE)
temp_mat <- (exp_scores_mat_div_sum_exp - exp_scores_mat_div_sum_exp^2) * rewards_mat
temp <- array(0, dim = c(n, p, k))
for (i in 1:n){
for (j in 1:p){
for (m in 1:k){
temp[i,j,k]=array(temp_mat, dim = c(n, 1, k))[i,1,m] * array(features_mat, dim = c(n, p, 1))[i,j,1]
}
}
}
grad_theta_mat <- apply(temp, c(2, 3), mean)
theta <- theta + (learning_rate) * (grad_theta_mat)
theta = t(t(theta)-theta[,1])
}
return(theta)
}
n <- nrow(features_mat)
p <- ncol(features_mat)
k <- ncol(rewards_mat)
theta <- matrix(1, nrow = p, ncol = k)
for (i in 1:10) {
scores_mat <- features_mat %*% theta
exp_scores_mat <- exp(scores_mat)
exp_scores_mat_div_sum_exp <- exp_scores_mat / rowSums(exp_scores_mat, na.rm = TRUE)
temp_mat <- (exp_scores_mat_div_sum_exp - exp_scores_mat_div_sum_exp^2) * rewards_mat
temp <- array(0, dim = c(n, p, k))
for (i in 1:n){
for (j in 1:p){
for (m in 1:k){
temp[i,j,k]=array(temp_mat, dim = c(n, 1, k))[i,1,m] * array(features_mat, dim = c(n, p, 1))[i,j,1]
}
}
}
grad_theta_mat <- apply(temp, c(2, 3), mean)
theta <- theta + (0.01) * (grad_theta_mat)
theta = t(t(theta)-theta[,1])
}
# Example Usage for convex_relaxation:
set.seed(123)
features_mat <- matrix(rnorm(250), ncol = 5)
beta <- matrix(rnorm(10), ncol = 2)
rewards_mat <- features_mat %*% beta+matrix(runif(100), ncol = 2)
theta_result_convex <- convex_relaxation(features_mat, rewards_mat)
print("Theta Result (Convex Relaxation):")
print(theta_result_convex)
theta_result_logistic <- logistic_policy(features_mat, rewards_mat)
print("Theta Result (theta_result_logistic):")
print(theta_result_logistic)
# 定义 logistic_policy 函数，该函数实现了一个简单的逻辑回归策略学习算法
logistic_policy <- function(features_mat, rewards_mat, max_iter = 1000,
learning_rate = 1, reg_param = 0, theta_init = NULL, intercept = TRUE) {
# 获取数据集的行数、特征数和动作数
n <- nrow(features_mat)
p <- ncol(features_mat)
k <- ncol(rewards_mat)
# 初始化参数矩阵 theta
if (is.null(theta_init)) {
theta <- matrix(0, nrow = p, ncol = k)
} else {
theta <- theta_init
}
# 迭代更新参数
for (i in 1:max_iter) {
# 计算分数矩阵
scores_mat <- features_mat %*% theta
# 计算指数分数矩阵
exp_scores_mat <- exp(scores_mat)
# 计算每行的指数分数占总和的比例
exp_scores_mat_div_sum_exp <- exp_scores_mat / rowSums(exp_scores_mat, na.rm = TRUE)
# 计算临时矩阵
temp_mat <- (exp_scores_mat_div_sum_exp - exp_scores_mat_div_sum_exp^2) * rewards_mat
# 初始化临时数组 temp
temp <- array(0, dim = c(n, p, k))
# 利用循环计算 temp 数组
for (i in 1:n) {
for (j in 1:p) {
for (m in 1:k) {
temp[i, j, k] = array(temp_mat, dim = c(n, 1, k))[i, 1, m] * array(features_mat, dim = c(n, p, 1))[i, j, 1]
}
}
}
# 计算梯度矩阵 grad_theta_mat
grad_theta_mat <- apply(temp, c(2, 3), mean)
# 更新参数 theta
theta <- theta + (learning_rate) * (grad_theta_mat)
# 对参数 theta 进行操作
theta = t(t(theta) - theta[, 1])
}
# 返回最终学到的参数矩阵 theta
return(theta)
}
# Example Usage for convex_relaxation:
set.seed(123)
features_mat <- matrix(rnorm(250), ncol = 5)
beta <- matrix(rnorm(10), ncol = 2)
rewards_mat <- features_mat %*% beta+matrix(runif(100), ncol = 2)
theta_result_convex <- convex_relaxation(features_mat, rewards_mat)
print("Theta Result (Convex Relaxation):")
print(theta_result_convex)
theta_result_logistic <- logistic_policy(features_mat, rewards_mat)
print("Theta Result (theta_result_logistic):")
print(theta_result_logistic)
devtools::document()
devtools::document()
devtools::document()
rlang::last_trace()
devtools::document()
rm(list = c("convex_relaxation", "gibbsSampler", "logistic_policy"))
devtools::document()
rm(list = c("gibbsSampler"))
devtools::document()
View(gibbsSampler)
View(gibbsSampler)
devtools::document()
devtools::document()
devtools::document()
gc()
library(Rcpp)
devtools::document()
library(SA23204180)
remove.packages("SA23204180")
library(SA23204180)
gibbsR()
devtools::document()
devtools::check()
library(boot)
devtools::check()
devtools::check()
devtools::document()
warnings()
devtools::document()
devtools::document()
?gibbsSampler
?gibbsSampler
rm(list = c("gibbsSampler"))
devtools::document()
View(gibbsSampler)
devtools::document()
devtools::document()
devtools::document()
devtools::build_vignettes()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
install.packages('../SA23204180.tar.gz',repo=NULL)
install.packages('../SA23204180_1.0.tar.gz',repo=NULL)
# 卸载已加载的包
detach("package:SA23204180", unload = TRUE)
# 然后重新安装
install.packages(pkgs = '../SA23204180.tar.gz', repos = NULL, type = 'source')
install.packages('../SA23204180_1.0.tar.gz',repo=NULL)
devtools::check()
devtools::build(vignettes=FALSE)
devtools::build_vignettes()
devtools::build(vignettes=FALSE)
install.packages('../SA23204180_1.0.tar.gz',repo=NULL)
install.packages('../SA23204180_1.0.tar.gz',repo=NULL)
