% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/StatCompR.R
\name{logistic_policy}
\alias{logistic_policy}
\title{Logistic Policy Function}
\usage{
logistic_policy(
  features_mat,
  rewards_mat,
  max_iter = 1000,
  learning_rate = 1,
  reg_param = 0,
  theta_init = NULL,
  intercept = TRUE
)
}
\arguments{
\item{features_mat}{A matrix representing the features of the dataset.}

\item{rewards_mat}{A matrix representing the rewards associated with different actions.}

\item{max_iter}{The maximum number of iterations for updating parameters (default is 1000).}

\item{learning_rate}{The learning rate for parameter updates (default is 1).}

\item{reg_param}{The regularization parameter (default is 0).}

\item{theta_init}{Initial values for the parameter matrix theta (default is NULL).}

\item{intercept}{Include intercept term in the logistic regression (default is TRUE).}
}
\value{
A matrix representing the learned parameters theta.
}
\description{
This function implements a simple logistic policy learning algorithm.
It iteratively updates the parameter matrix theta based on the features and rewards matrices.
}
\examples{
\dontrun{
set.seed(123)
features_mat <- matrix(rnorm(250), ncol = 5)
beta <- matrix(rnorm(10), ncol = 2)
rewards_mat <- features_mat \%*\% beta + matrix(runif(100), ncol = 2)
theta_result_logistic <- logistic_policy(features_mat, rewards_mat)
print("Theta Result (Logistic Policy):")
print(theta_result_logistic)
}

}
